{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1tZS9JrjSyA6xMc2RtTy9rrrc-tDTf6XV",
      "authorship_tag": "ABX9TyPyOBy2PwIwtGcjodR/mNYr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/clark1031/australian-ai-salary-analysis/blob/main/australian_ai_salary_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iBvgAf7tKRTx"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# A Predictive and Interpretive Analysis of Salary Drivers\n",
        "# in the Australian AI Skills Landscape\n",
        "#\n",
        "# Author: Jiansong Zhang\n",
        "# Course: Big Data Analysis and Project\n",
        "# Date: July 2025\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Import Core Libraries ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import kagglehub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# PHASE 1: DATA SOURCING, FILTERING & PRE-PROCESSING\n",
        "# ==============================================================================\n",
        "print(\"--- Phase 1: Data Sourcing, Filtering & Pre-processing ---\")\n",
        "\n",
        "# --- Step 1.1: Authenticate and Load Data from Kaggle ---\n",
        "# This step requires the user to upload their kaggle.json API key.\n",
        "# try:\n",
        "#     from google.colab import files\n",
        "#     print(\"Please upload your kaggle.json file:\")\n",
        "#     files.upload()\n",
        "#     !mkdir -p ~/.kaggle\n",
        "#     !mv kaggle.json ~/.kaggle/\n",
        "#     !chmod 600 ~/.kaggle/kaggle.json\n",
        "#     kagglehub.login()\n",
        "#     print(\"Kaggle Hub Authenticated.\")\n",
        "# except ImportError:\n",
        "#     print(\"Not in a Colab environment. Assuming local Kaggle setup.\")\n",
        "\n",
        "# Load the dataset using kagglehub\n",
        "# NOTE: For kagglehub execution without local, replace this block with:\n",
        "# print(\"\\nLoading dataset from Kaggle...\")\n",
        "# df = kagglehub.load_dataset(\n",
        "#     \"bismasajjad/global-ai-job-market-and-salary-trends-2025\",\n",
        "#     file_path=\"ai_job_dataset.csv\",\n",
        "# )\n",
        "df = pd.read_csv('path/to/your/ai_job_dataset.csv')\n",
        "print(f\"Dataset loaded successfully. Shape: {df.shape}\")\n",
        "\n",
        "# --- Step 1.2: Filter for Australian Data and Initial Cleaning ---\n",
        "# Create the Australian subset\n",
        "df_au = df[df['company_location'] == 'Australia'].copy()\n",
        "\n",
        "# Drop rows with missing values in critical columns\n",
        "df_cleaned_au = df_au.dropna(subset=['required_skills', 'salary_usd']).copy()\n",
        "print(f\"Filtered to {len(df_cleaned_au)} clean Australian records.\")"
      ],
      "metadata": {
        "id": "C9lg6IrRKYFN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# PHASE 2: EXPLORATORY DATA ANALYSIS (EDA) & FEATURE ENGINEERING\n",
        "# ==============================================================================\n",
        "print(\"\\n--- Phase 2: EDA & Feature Engineering ---\")\n",
        "\n",
        "# --- Step 2.1: Skill Clustering (Feature Engineering) ---\n",
        "# Define a mapping to group specific skills into broader categories\n",
        "skill_map = {\n",
        "    'tableau': 'business_intelligence', 'power bi': 'business_intelligence', 'data visualization': 'business_intelligence',\n",
        "    'aws': 'cloud_computing', 'gcp': 'cloud_computing', 'azure': 'cloud_computing',\n",
        "    'pytorch': 'deep_learning_framework', 'tensorflow': 'deep_learning_framework', 'keras': 'deep_learning_framework',\n",
        "    'docker': 'containerization', 'kubernetes': 'containerization',\n",
        "    'nlp': 'natural_language_processing', 'computer vision': 'computer_vision'\n",
        "}\n",
        "\n",
        "def apply_skill_mapping(skill_list):\n",
        "    \"\"\"Maps a list of raw skills to their higher-level categories.\"\"\"\n",
        "    mapped_set = set()\n",
        "    for skill in skill_list:\n",
        "        mapped_set.add(skill_map.get(skill, skill))\n",
        "    return list(mapped_set)\n",
        "\n",
        "# Apply the skill clustering to the Australian data\n",
        "df_cleaned_au['skills_list'] = df_cleaned_au['required_skills'].str.lower().str.split(r',\\s*')\n",
        "df_cleaned_au['skills_mapped'] = df_cleaned_au['skills_list'].apply(apply_skill_mapping)\n",
        "print(\"\\nSkill clustering applied.\")\n",
        "\n",
        "# --- Step 2.2: EDA Visualizations ---\n",
        "print(\"Generating EDA visualizations...\")\n",
        "sns.set_style(\"whitegrid\")\n",
        "\n",
        "# --- New Skill Counting Function ---\n",
        "def get_mapped_skill_counts(dataframe):\n",
        "    \"\"\"\n",
        "    Takes a DataFrame with a 'skills_mapped' column and returns\n",
        "    a Series with the counts of each mapped skill.\n",
        "    \"\"\"\n",
        "    return dataframe['skills_mapped'].explode().value_counts()\n",
        "\n",
        "mapped_au_skill_counts = get_mapped_skill_counts(df_cleaned_au)\n",
        "top_10_australia_mapped = mapped_au_skill_counts.head(10)\n",
        "\n",
        "# Visualization 1: Top 10 Mapped Skills in Australia\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.barplot(x=top_10_australia_mapped.values, y=top_10_australia_mapped.index)\n",
        "plt.title('Figure 1: Top 10 Most In-Demand Skill Categories in Australia', fontsize=16)\n",
        "plt.xlabel('Number of Job Postings', fontsize=12)\n",
        "plt.ylabel('Skill Category', fontsize=12)\n",
        "plt.show()\n",
        "\n",
        "# Visualization 2: Salary Distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df_cleaned_au['salary_usd'], kde=True, bins=30)\n",
        "plt.title('Fig. A: Distribution of AI Salaries in Australia (USD)', fontsize=16)\n",
        "plt.xlabel('Annual Salary (USD)', fontsize=12)\n",
        "plt.ylabel('Frequency', fontsize=12)\n",
        "plt.show()\n",
        "\n",
        "# Visualization 3: Salary by Experience Level\n",
        "plt.figure(figsize=(10, 7))\n",
        "exp_order = ['EN', 'MI', 'SE', 'EX']\n",
        "sns.boxplot(data=df_cleaned_au, x='experience_level', y='salary_usd', order=exp_order)\n",
        "plt.title('Fig. B: Salary Distribution by Experience Level in Australia', fontsize=16)\n",
        "plt.xlabel('Experience Level (Entry, Mid, Senior, Executive)', fontsize=12)\n",
        "plt.ylabel('Annual Salary (USD)', fontsize=12)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "osRPOxnXKcA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# PHASE 3: FINAL FEATURE PREPARATION FOR MODELLING\n",
        "# ==============================================================================\n",
        "print(\"\\n--- Phase 3: Final Feature Preparation for Modelling ---\")\n",
        "\n",
        "# --- Step 3.1: One-Hot Encode Categorical Features ---\n",
        "df_model_ready = pd.get_dummies(df_cleaned_au, columns=['experience_level'], prefix='exp', drop_first=True)\n",
        "print(\"One-hot encoded 'experience_level'.\")\n",
        "\n",
        "# --- Step 3.2: Create Binary Skill Features ---\n",
        "top_skills_for_model = ['cloud_computing', 'deep_learning_framework', 'business_intelligence', 'containerization']\n",
        "for skill in top_skills_for_model:\n",
        "    df_model_ready[f'has_{skill}'] = df_model_ready['skills_mapped'].apply(lambda skills: 1 if skill in skills else 0)\n",
        "print(\"Created binary features for top skills.\")\n",
        "\n",
        "# --- Step 3.3: Define Final Feature Sets and Target Variable ---\n",
        "y = df_model_ready['salary_usd']\n",
        "feature_columns_full = ['years_experience', 'exp_MI', 'exp_SE', 'exp_EX'] + [f'has_{skill}' for skill in top_skills_for_model]\n",
        "X_full = df_model_ready[feature_columns_full]\n",
        "\n",
        "# --- Step 3.4: Split Data into Training and Testing Sets ---\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_full, y, test_size=0.2, random_state=42)\n",
        "print(f\"\\nData split into {len(X_train)} training records and {len(X_test)} testing records.\")"
      ],
      "metadata": {
        "id": "pGJNz_dIKgF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# PHASE 4: PREDICTIVE MODELLING & EVALUATION\n",
        "# ==============================================================================\n",
        "print(\"\\n--- Phase 4: Predictive Modelling & Evaluation ---\")\n",
        "\n",
        "from sklearn.linear_model import LinearRegression, Ridge\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import r2_score, mean_absolute_error\n",
        "\n",
        "# --- Step 4.1: Train and Evaluate Initial (Default) Models ---\n",
        "models_initial = {\n",
        "    \"Linear Regression\": LinearRegression(),\n",
        "    \"Ridge Regression (Default)\": Ridge(alpha=1.0),\n",
        "    \"Random Forest (Default)\": RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "}\n",
        "\n",
        "results = []\n",
        "for name, model in models_initial.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    results.append({\n",
        "        \"Model\": name,\n",
        "        \"R-squared\": r2_score(y_test, y_pred),\n",
        "        \"Mean Absolute Error (MAE)\": mean_absolute_error(y_test, y_pred)\n",
        "    })\n",
        "print(\"\\nInitial models trained and evaluated.\")\n",
        "\n",
        "# --- Step 4.2: Hyperparameter Tuning for Refined Models ---\n",
        "print(\"Performing hyperparameter tuning...\")\n",
        "\n",
        "# Tune Ridge Regression\n",
        "param_grid_ridge = {'alpha': [0.1, 1.0, 10.0, 100.0]}\n",
        "grid_search_ridge = GridSearchCV(Ridge(), param_grid_ridge, cv=5, scoring='r2')\n",
        "grid_search_ridge.fit(X_train, y_train)\n",
        "tuned_ridge_model = grid_search_ridge.best_estimator_\n",
        "print(f\"Best alpha for Ridge Regression found: {grid_search_ridge.best_params_['alpha']}\")\n",
        "\n",
        "# Tune Random Forest Regressor\n",
        "param_grid_rf = {'n_estimators': [100, 200], 'max_depth': [10, None], 'max_features': ['sqrt']}\n",
        "grid_search_rf = GridSearchCV(RandomForestRegressor(random_state=42), param_grid_rf, cv=3, scoring='r2', n_jobs=-1)\n",
        "grid_search_rf.fit(X_train, y_train)\n",
        "tuned_rf_model = grid_search_rf.best_estimator_\n",
        "print(f\"Best parameters for Random Forest found: {grid_search_rf.best_params_}\")\n",
        "\n",
        "# --- Step 4.3: Evaluate Tuned Models ---\n",
        "models_tuned = {\n",
        "    \"Ridge Regression (Tuned)\": tuned_ridge_model,\n",
        "    \"Random Forest (Tuned)\": tuned_rf_model\n",
        "}\n",
        "\n",
        "for name, model in models_tuned.items():\n",
        "    y_pred = model.predict(X_test)\n",
        "    results.append({\n",
        "        \"Model\": name,\n",
        "        \"R-squared\": r2_score(y_test, y_pred),\n",
        "        \"Mean Absolute Error (MAE)\": mean_absolute_error(y_test, y_pred)\n",
        "    })\n",
        "print(\"\\nTuned models evaluated.\")\n",
        "\n",
        "# --- Step 4.4: Display Comprehensive Performance Comparison ---\n",
        "df_performance = pd.DataFrame(results)\n",
        "# Formatting for display\n",
        "df_performance['R-squared'] = df_performance['R-squared'].map('{:.3f}'.format)\n",
        "df_performance['Mean Absolute Error (MAE)'] = df_performance['Mean Absolute Error (MAE)'].map('${:,.0f}'.format)\n",
        "\n",
        "print(\"\\n--- Comprehensive Model Performance Comparison on Test Set ---\")\n",
        "print(df_performance.to_string())"
      ],
      "metadata": {
        "id": "OO1ZxVkuKj7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# PHASE 5: INTERPRETATION OF BEST MODEL\n",
        "# ==============================================================================\n",
        "print(\"\\n--- Phase 5: Interpretation of Best Model (Linear Regression) ---\")\n",
        "\n",
        "# --- Step 5.1: Analyze Multicollinearity in Experience Features ---\n",
        "experience_features = ['years_experience', 'exp_MI', 'exp_SE', 'exp_EX']\n",
        "experience_correlation = df_model_ready[experience_features].corr()\n",
        "print(\"\\n--- Correlation Matrix of Experience-Related Features ---\")\n",
        "print(experience_correlation)\n",
        "\n",
        "# --- Step 5.2: Extract and Display Model Coefficients ---\n",
        "# We use the initial Linear Regression model for interpretation as it is unpenalized\n",
        "lr_model = models_initial[\"Linear Regression\"]\n",
        "coefficients = pd.DataFrame(lr_model.coef_, X_full.columns, columns=['Coefficient (Monetary Value)'])\n",
        "print(\"\\n--- Multiple Linear Regression Model Coefficients ---\")\n",
        "print(coefficients)\n",
        "\n",
        "print(\"\\n--- End of Analysis ---\")"
      ],
      "metadata": {
        "id": "raU1ybjSKqX6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}